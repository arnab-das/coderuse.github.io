<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@coderuse"><meta name="twitter:title" content="Configure GitLab backup into Amazon S3"><meta name="twitter:description" content="&lt;p&gt;In the &lt;a href=&quot;/2016/09/Installation-And-Configuration-Of-GitLab-Server-With-CI/&quot;&gt;previous post&lt;/a&gt;, I tried to write a step by step procedure to install and configure a &lt;a href=&quot;https://about.gitlab.com/downloads/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitLab Community Edition Server&lt;/a&gt;. Now, whenever we are running a server with some kind of hosting, whatever that may be, site-hosting, file-hosting, image-hosting or any other kind of hosting, proper backup is needed. Whatever we do to protect our server, for a cloud vps, it’s unlikely, but it may crash, or for some un-known reason, hosting account may be suspended, or host may go down for bankruptsy or server’s IP may get banned and the site can’t be accessed due to the firewall of the ISP. So, there are many reasons, that can result in no-access-to-the-server. And as a precaution, we need to take backup of the server regularly.&lt;/p&gt;"><meta name="twitter:image" content="https://gitlab.com/gitlab-com/gitlab-artwork/raw/master/wordmark/stacked_wm.png"><title>Configure GitLab backup into Amazon S3 | CodeRuse</title><link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Open+Sans:300|Roboto:300|Inconsolata"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="amphtml" href="./amp/index.html"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">CodeRuse</a><!--span.subtitle= config.subtitle--><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><div id="top-nav" class="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" target="_blank" class="sidebar-nav-item">Archives</a><a href="http://galleries.coderuse.com" target="_blank" class="sidebar-nav-item">Galleries</a><a href="/about" target="_blank" class="sidebar-nav-item">About</a></div></div><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Configure GitLab backup into Amazon S3</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">Sep 19, 2016</div><div class="post-categories"><a class="post-category-link" href="/categories/CI/">CI</a>><a class="post-category-link" href="/categories/CI/Cookbook/">Cookbook</a>><a class="post-category-link" href="/categories/CI/Cookbook/System/">System</a>><a class="post-category-link" href="/categories/CI/Cookbook/System/Technical/">Technical</a>><a class="post-category-link" href="/categories/CI/Cookbook/System/Technical/Economical/">Economical</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/ci/">ci</a>/<a class="post-tag-link" href="/tags/gitlab/">gitlab</a>/<a class="post-tag-link" href="/tags/private-git/">private-git</a></div></div></div><article><div class="container post"><p>In the <a href="/2016/09/Installation-And-Configuration-Of-GitLab-Server-With-CI/">previous post</a>, I tried to write a step by step procedure to install and configure a <a href="https://about.gitlab.com/downloads/" target="_blank" rel="external">GitLab Community Edition Server</a>. Now, whenever we are running a server with some kind of hosting, whatever that may be, site-hosting, file-hosting, image-hosting or any other kind of hosting, proper backup is needed. Whatever we do to protect our server, for a cloud vps, it’s unlikely, but it may crash, or for some un-known reason, hosting account may be suspended, or host may go down for bankruptsy or server’s IP may get banned and the site can’t be accessed due to the firewall of the ISP. So, there are many reasons, that can result in no-access-to-the-server. And as a precaution, we need to take backup of the server regularly.</p>
<p>For GitLab, backup means backup of configuration and application data both. <a href="https://docs.gitlab.com/omnibus/settings/backups.html" target="_blank" rel="external">More details</a>.</p>
<p>To take backup, first we need to decide on the policy regarding storing the backup. Yes this is very important.</p>
<p>It’s possible to take backup regularly, download the archive files to local computer and keep the archives in a convenient place. But, it’s good to have an automated process for this. And we should keep in mind that, our local machines can crash and if that happens, all the backups will go to vein. For this reason, we should select providers like <a href="https://aws.amazon.com/s3/" target="_blank" rel="external">AWS S3</a>, <a href="https://cloud.google.com/storage/" target="_blank" rel="external">Google CLoud Storage</a>, <a href="https://www.rackspace.com/cloud/files" target="_blank" rel="external">RackSpace Cloud Files</a> to store our backups. They have disater recovery processes and there is almost no chance that any file uploaded to these services, will be lost, unless we explicitly do this. GitLab depends on <a href="http://fog.io/" target="_blank" rel="external">Fog</a> for uploading backups to remote locations. And <code>Fog</code> supports only the above three cloud providers along with local storage. By comparing the pricing of the above three and considering the fact, mostly we will just upload and store the files, downloading will not happen very often, found that, AWS S3 has the least pricing. Refer to, <a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="external">AWS S3 pricing</a>, <a href="https://cloud.google.com/storage/pricing" target="_blank" rel="external">Google Cloud Storage pricing</a> and <a href="https://www.rackspace.com/cloud/files" target="_blank" rel="external">RackSpace Cloud Files pricing</a>. Observe that, there are three types of storage in AWS S3 and Google Cloud Storage. We can decide that, initially we will upload the files to <code>Standard Storage</code> and after some time move the files to <code>Glacier Storage</code> of AWS or <code>Nearline Storage</code> of Google and after a long period of time, say one year, we will actuallly delete the files. Beacause after such a long period, there will not remain any relevance of such old backup. In S3, we can explicitly set such rule. We will see.</p>
<p>I’ve used AWS S3 to preserve the backed up data. If you want to use any other provider, this guide may not help you that much.</p>
<p>Let’s start.</p>
<p>Create one AWS account. If you’re an Amazon customer, you may either link the AWS account with the exisiting customer account or create a fresh one with new email. Amazon also gives a <a href="https://aws.amazon.com/free/" target="_blank" rel="external">12 month free tier</a>, upon signup of a new account. Here one credit or debit card <a href="https://payments.amazon.com/help/201754650" target="_blank" rel="external">eligible</a> for online transaction is required. Open <a href="https://aws.amazon.com/" target="_blank" rel="external">AWS Console</a>, click on the link/button <strong>Sign In to the Console</strong> and complete the procedure to sign up and in the last page click on the link, something like <strong>Sign In to Console</strong>. Check the email in the username box and select the <em>returning user</em> radio button and enter the password.</p>
<p>To create a <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html" target="_blank" rel="external">bucket</a>, think <code>bucket</code> as a bowl to put your backups, go <a href="https://console.aws.amazon.com/s3/home" target="_blank" rel="external">here</a>. Click on the <strong>Create Bucket</strong> button. Give the bucket name, you decided and select a region (here choice of region doesn’t matter seriously unless you have some constraint), and if you need logging, click on <strong>Enable Logging</strong>, otherwise click on <strong>Create</strong>. Your bucket is ready to take files.</p>
<p>Here we have to set the <code>Lifecycle</code> of the files. Click on the bucket, just created and click on the <strong>Properties</strong> button if not selected in prior. Click <strong>Lifecycle</strong> and create one rule. The wizard is self explaining. Fill it, at your own choice. I’ve set that, after 30 days, files would move to <code>Glacier Storage</code> and after 365 days from the creation time the files will be deleted. Now, let’s move towards to create users to configure the automated backup process.</p>
<p>It’s not recommended to use the root user, the user, you have logged with, in any of the automated tasks. Because if someone with malicious intent gets access to our server, severe damage can happen. So, create one IAM user <a href="https://console.aws.amazon.com/iam/home" target="_blank" rel="external">here</a>. Click on the <strong>Users</strong> tab to the left and click on <strong>Create New User</strong>. You can create upto 5 users at a time. Here we need only one user. Give the username. Keep the checkbox <code>Generate an access key for each user</code> checked. Click <strong>Create</strong> and in the next page download the credential by clicking on the button <strong>Download Credentials</strong> and then click <strong>Close</strong>. It’ll be better to create a group with proper inline policy, so that, it’ll be easier to manage user permissions.</p>
<p>Click on the <strong>Groups</strong> tab to the left. And create one group. Don’t attach any policy here. Just create the group. After creation, click on the group to open the details. Click on the <strong>Permissions</strong> and open the accordion <strong>Inline Policies</strong>. Click on the <strong>Click Here</strong> link. And select the button corresponding to <strong>Policy Generator</strong>. Select <code>S3</code> from the dropdown of <code>AWS Service</code>. And select the following actions for specified resources. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Resource - &#34;arn:aws:s3:::&#60;bucket-name&#62;/*&#34; &#10;&#34;s3:AbortMultipartUpload&#34;,&#10;&#34;s3:GetBucketAcl&#34;,&#10;&#34;s3:GetBucketLocation&#34;,&#10;&#34;s3:GetObject&#34;,&#10;&#34;s3:GetObjectAcl&#34;,&#10;&#34;s3:ListBucketMultipartUploads&#34;,&#10;&#34;s3:PutObject&#34;,&#10;&#34;s3:PutObjectAcl&#34;&#10;&#10;# Resource - &#34;*&#34;&#10;&#34;s3:GetBucketLocation&#34;,&#10;&#34;s3:ListAllMyBuckets&#34;&#10;&#10;# Resource - &#34;arn:aws:s3:::&#60;bucket-name&#62;&#34;&#10;&#34;s3:ListBucket&#34;</span><br></pre></td></tr></table></figure>
<p>After putting the resource name each time click on <strong>Add Statement</strong> and finally click on <strong>Next Step</strong> and give a name of the policy and create it. The policy will be listed in the <strong>Permission</strong> tab of the Group.</p>
<p>Click on the <strong>Users</strong> tab in the group details page. Click <strong>Add Users to Group</strong> to add the user created previously.</p>
<p>Now we have to configure our GitLab server to upload the backups to AWS. Open <code>/etc/gitlab/gitlab.rb</code> in your favourite editor and add the search for the text <code>manage_backup_path</code> and make the block like below. <a href="https://docs.gitlab.com/ce/raketasks/backup_restore.html" target="_blank" rel="external">More Details</a>.</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">gitlab_rails[<span class="string">'manage_backup_path'</span>] = <span class="keyword">true</span></span><br><span class="line">gitlab_rails[<span class="string">'backup_path'</span>] = <span class="string">"/path/to/backup/location/in/server"</span></span><br><span class="line"><span class="comment"># gitlab_rails['backup_archive_permissions'] = 0644 # See: http://doc.gitlab.com/ce/raketasks/backup_restore.html#backup-archive-permissions</span></span><br><span class="line"><span class="comment"># gitlab_rails['backup_pg_schema'] = 'public'</span></span><br><span class="line"><span class="comment"># gitlab_rails['backup_keep_time'] = 604800</span></span><br><span class="line">gitlab_rails[<span class="string">'backup_upload_connection'</span>] = &#123;</span><br><span class="line">   <span class="string">'provider'</span> =&gt; <span class="string">'AWS'</span>,</span><br><span class="line">   <span class="string">'region'</span> =&gt; <span class="string">'us-west-2'</span>,</span><br><span class="line">   <span class="string">'aws_access_key_id'</span> =&gt; <span class="string">'_secret_id_'</span>,</span><br><span class="line">   <span class="string">'aws_secret_access_key'</span> =&gt; <span class="string">'_secret_key_'</span></span><br><span class="line">&#125;</span><br><span class="line">gitlab_rails[<span class="string">'backup_upload_remote_directory'</span>] = <span class="string">'bucket-name'</span></span><br><span class="line">gitlab_rails[<span class="string">'backup_multipart_chunk_size'</span>] = <span class="number">104857600</span></span><br><span class="line">gitlab_rails[<span class="string">'backup_encryption'</span>] = <span class="string">'AES256'</span> <span class="comment"># Turns on AWS Server-Side Encryption with Amazon S3-Managed Keys for backups</span></span><br></pre></td></tr></table></figure>
<p>For the codename of AWS regions, refer to <a href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region" target="_blank" rel="external">this</a>. AWS secret access key id and access key can be found in the downloaded credential file, when the IAM user was created. Replace <code>bucket-name</code> with proper name of the bucket, you had created in prior.</p>
<p>Almost done. Now, we have to run only two commands.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reconfigure GitLab with the modified configurations</span></span><br><span class="line">sudo gitlab-ctl reconfigure</span><br><span class="line"></span><br><span class="line"><span class="comment"># create first backup of the server</span></span><br><span class="line">sudo gitlab-rake gitlab:backup:create</span><br></pre></td></tr></table></figure>
<p>If all the steps have configured correctly, you should be able to see a <code>.tar</code> file in the web view of the bucket.</p>
<p>But, we don’t want to take backups manually. So, we will set a <a href="https://en.wikipedia.org/wiki/Cron" target="_blank" rel="external">cronjob</a> in our server. Invoke the command <code>sudo crontab -e -u root</code>, or you may want specify the useid, running the GitLab server. If the crontab has not been configured, it will ask for the default editor, you want to use. Select one by giving one number. Then the <code>cron-file</code> will be opened in the editor selected.</p>
<p>Now, we have to understand a little bit about the syntax of the cron. It’s very easy. Just read <a href="http://kvz.io/blog/2007/07/29/schedule-tasks-on-linux-using-crontab/" target="_blank" rel="external">this</a> once. By the knowledge, we just acquired, let’s add one line to the end of the cron file and save the file. Cron will automatically create one cronjob for it.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># it says that run the backup command on the 7th day, i.e. &#10;# Saturday, depends on the server setting, at 23:59 hrs&#10;# and the backup command will take care of creating backups&#10;# and uploading them to AWS S3&#10;59 23 * * 7 /usr/bin/gitlab-rake gitlab:backup:create</span><br></pre></td></tr></table></figure>
<p>It completes the automation of taking backups of application data, i.e. repositories, databases etc.</p>
<p>We are not finished yet. We need to take backup of the <code>/etc/gitlab</code> folder also. But, as this folder does not change very frequently, decided to take backup of this folder manually. Specially if you have many users you may want to setup a cron job to create archives of this folder at regular intervals. Refer to <a href="https://docs.gitlab.com/omnibus/settings/backups.html" target="_blank" rel="external">this</a>.</p>
<p>Code happily and be happy. :-)</p>
</div><!-- comment system--><div class="container"><div id="disqus_thread"></div><script type="text/javascript">
var disqus_shortname = 'coderuse';
var disqus_identifier = '2016/09/Configure-GitLab-Backup-Into-Amazon-S3/';
var disqus_title = "Configure GitLab backup into Amazon S3";
var disqus_url = 'http://coderuse.com/2016/09/Configure-GitLab-Backup-Into-Amazon-S3/';
(function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">Blog comments powered by <span class="logo-disqus">Disqus</span></a></div></article><div id="footer"><div class="container"><div class="bar"><div class="social"><a href="http://twitter.com/coderuse" target="_blank"><i class="fa fa-twitter"></i></a><a href="https://github.com/coderuse" target="_blank"><i class="fa fa-github"></i></a><a href="https://plus.google.com/+ArnabDasUIDeveloper" target="_blank"><i class="fa fa-google-plus"></i></a><a href="/atom.xml" target="_blank"><i class="fa fa-rss"></i></a></div><div class="footer">© <a href="/" rel="nofollow">CodeRuse</a> | <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0</a></div></div></div></div><script>var trackID = 'UA-70816071-1', domainID = 'coderuse.com';(function (i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r] || function(){(i[r].q=i[r].q || []).push(arguments);},i[r].l = 1 * new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.defer=1;a.src=g;m.parentNode.insertBefore(a,m)})(window, document,'script','//www.google-analytics.com/analytics.js','ga');ga('create', trackID, domainID);ga('send', 'pageview');</script></body></html>